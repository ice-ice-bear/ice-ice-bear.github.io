layout: single
title: "[혼공머신]4-2 -추가학습(기타 손실함수)"
categories: machine-learning
tag: [python, machine-learning]
use_math: true
toc: True ##Table of contents
typora-root-url: ../ 

## 기타 손실함수

### 1. **Hinge Loss (힌지 손실 함수)**

#### 특징:
- 주로 서포트 벡터 머신(SVM)에서 사용됩니다.
- 분류 문제에 적합하며, 데이터 포인트가 결정 경계를 넘어갈 때 손실을 가중합니다.
- 이진 분류 문제에서 많이 사용됩니다.

#### 예시:
```python
from sklearn.linear_model import SGDClassifier

clf = SGDClassifier(loss='hinge', max_iter=1000, tol=0.001, random_state=42)
clf.fit(train_scaled, train_target)
print("훈련 세트 점수:", clf.score(train_scaled, train_target))
print("테스트 세트 점수:", clf.score(test_scaled, test_target))
```

### 2. **Squared Hinge Loss (제곱 힌지 손실 함수)**

#### 특징:
- 힌지 손실의 변형으로, 제곱된 값을 사용하여 오류가 클 때 더 큰 패널티를 줍니다.
- 더 큰 마진을 제공하여 분류기의 일반화 능력을 향상시킵니다.

#### 예시:
```python
clf = SGDClassifier(loss='squared_hinge', max_iter=1000, tol=0.001, random_state=42)
clf.fit(train_scaled, train_target)
print("훈련 세트 점수:", clf.score(train_scaled, train_target))
print("테스트 세트 점수:", clf.score(test_scaled, test_target))
```

### 3. **Huber Loss (후버 손실 함수)**

#### 특징:
- 회귀 문제에서 사용됩니다.
- 작은 오류에 대해서는 제곱 오차로 처리하고, 큰 오류에 대해서는 절대 오차로 처리하여 이상치에 강건한 성능을 보입니다.

#### 예시:
```python
from sklearn.linear_model import SGDRegressor

reg = SGDRegressor(loss='huber', max_iter=1000, tol=0.001, random_state=42)
reg.fit(train_scaled, train_target)
print("훈련 세트 점수:", reg.score(train_scaled, train_target))
print("테스트 세트 점수:", reg.score(test_scaled, test_target))
```

### 4. **Log Loss (로그 손실 함수 또는 로지스틱 손실 함수)**

#### 특징:
- 로지스틱 회귀에서 사용됩니다.
- 이진 분류 문제에 적합하며, 예측 확률과 실제 클래스 간의 차이를 측정합니다.

#### 예시:
```python
clf = SGDClassifier(loss='log', max_iter=1000, tol=0.001, random_state=42)
clf.fit(train_scaled, train_target)
print("훈련 세트 점수:", clf.score(train_scaled, train_target))
print("테스트 세트 점수:", clf.score(test_scaled, test_target))
```

### 5. **Epsilon-Insensitive Loss (엡실론 민감 손실 함수)**

#### 특징:
- 서포트 벡터 회귀(SVR)에서 사용됩니다.
- 예측 값과 실제 값이 일정 범위(엡실론) 내에 있을 때 손실을 무시합니다.
- 회귀 문제에서 특정 오차 범위 내의 값을 무시하고, 이상치에 민감하지 않은 모델을 만듭니다.

#### 예시:
```python
reg = SGDRegressor(loss='epsilon_insensitive', max_iter=1000, tol=0.001, random_state=42)
reg.fit(train_scaled, train_target)
print("훈련 세트 점수:", reg.score(train_scaled, train_target))
print("테스트 세트 점수:", reg.score(test_scaled, test_target))
```

### 6. **Squared Loss (제곱 오차 손실 함수)**

#### 특징:
- 회귀 문제에서 가장 일반적으로 사용됩니다.
- 예측 값과 실제 값의 차이를 제곱하여 손실을 계산합니다.
- 이상치에 민감할 수 있습니다.

#### 예시:
```python
reg = SGDRegressor(loss='squared_loss', max_iter=1000, tol=0.001, random_state=42)
reg.fit(train_scaled, train_target)
print("훈련 세트 점수:", reg.score(train_scaled, train_target))
print("테스트 세트 점수:", reg.score(test_scaled, test_target))
```

### 7. **Perceptron Loss (퍼셉트론 손실 함수)**

#### 특징:
- 퍼셉트론 학습에서 사용됩니다.
- 분류 문제에 적합하며, 예측이 틀렸을 때만 손실을 계산합니다.

#### 예시:
```python
clf = SGDClassifier(loss='perceptron', max_iter=1000, tol=0.001, random_state=42)
clf.fit(train_scaled, train_target)
print("훈련 세트 점수:", clf.score(train_scaled, train_target))
print("테스트 세트 점수:", clf.score(test_scaled, test_target))
```

### 결론

이와 같이 다양한 손실 함수는 각기 다른 문제와 목표에 맞춰 최적의 성능을 발휘할 수 있도록 설계되었습니다. 각 손실 함수의 특성을 이해하고 적절히 선택하는 것이 모델 성능 향상의 핵심입니다.
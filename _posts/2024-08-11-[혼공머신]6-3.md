---
layout: single
title: "[혼공머신]6-3"
categories: machine-learning
tag: [python, machine-learning]
use_math: true
toc: True ##Table of contents
typora-root-url: ../ 
---

## 주성분 분석

### 1. 차원과 차원 축소

#### 차원의 개념
- **차원**이란 데이터에서 각 특성(Feature)의 수를 의미합니다. 예를 들어, 사람의 키, 몸무게, 나이라는 3가지 특성을 가진 데이터는 3차원 데이터입니다.

#### 차원 축소의 필요성
- **차원의 저주**: 차원이 증가할수록, 각 차원에 걸쳐 데이터를 충분히 표현하기 위해 필요한 데이터 양이 기하급수적으로 증가합니다. 이로 인해 모델의 성능이 저하될 수 있습니다.
- **계산 효율성 및 시각화**: 높은 차원의 데이터를 처리하는 것은 계산상의 비효율을 초래하며, 3차원 이상의 데이터는 직관적으로 이해하고 시각화하기 어렵습니다.

### 2. 주성분 분석 소개

#### PCA의 원리
- 주성분 분석은 데이터의 분산을 최대화하는 축을 찾아 데이터를 새로운 축에 투영함으로써 차원을 축소합니다. 이 때 새로운 축들은 서로 직교합니다.

#### 주성분(Principal Components)
- 데이터의 분산이 최대인 방향을 찾아 그 방향으로 데이터를 투영한 것이 주성분입니다. 첫 번째 주성분은 데이터의 분산을 가장 많이 설명하고, 두 번째 주성분은 첫 번째 주성분에 직각이면서 다음으로 큰 분산을 설명하는 방향입니다.

```python
# 필요한 라이브러리를 불러옵니다.
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# Iris 데이터셋을 로드합니다.
data = load_iris()
X = data.data

# PCA 모델을 생성하고 학습합니다. 이 때, 2개의 주성분만을 유지하도록 설정합니다.
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# 변환된 데이터의 분포를 시각화합니다.
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=data.target, cmap='viridis')
plt.xlabel('First principal component')
plt.ylabel('Second principal component')
plt.title('PCA result of Iris Dataset')
plt.colorbar()
plt.show()
```

### 3. PCA 클래스 (`scikit-learn`)

#### 주요 매개변수
- `n_components`: 축소할 차원의 수를 지정합니다. 기본적으로는 None으로 설정되며, 이 경우 더 적은 수의 주성분으로 설정됩니다.
- `random_state`: 결과의 일관성을 위한 난수 시드를 설정합니다.

#### 주요 속성
- `components_`: 추출된 주성분의 방향 벡터입니다.
- `explained_variance_`: 각 주성분에 의해 설명된 분산의 양입니다.
- `explained_variance_ratio_`: 전체 분산에 대한 각 주성분의 분산 비율입니다.

```python
# PCA 객체의 주요 속성을 출력합니다.
print("Components (Principal axes):", pca.components_)
print("Explained variance:", pca.explained_variance_)
print("Explained variance ratio:", pca.explained_variance_ratio_)
```

### 4. 원본 데이터 재구성

- PCA 변환 후, `inverse_transform()` 메서드를 사용하여 축소된 차원에서 원본 차원으로 데이터를 복원할 수 있습니다. 이 과정에서 일부 정보 손실이 발생하지만, 주요 특성은 유지됩니다.

```python
# 차원 축소된 데이터를 원래의 차원으로 복원합니다.
X_inverse = pca.inverse_transform(X_pca)

# 원본 데이터와 복원된 데이터의 비교를 위해 시각화합니다.
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], alpha=0.2, label='Original')
plt.scatter(X_inverse[:, 0], X_inverse[:, 1], alpha=0.8, label='Recovered')
plt.legend()
plt.title('Comparison of Original and Recovered Data')
plt.show()
```

### 5. 설명된 분산

- 설명된 분산은 PCA에서 각 주성분이 데이터 전체 분산에서 얼마나 많은 부분을 차지하는지를 나타내는 지표입니다. 높은 설명된 분산 비율은 주성분이 데이터의 중요한 정보를 많이 포함하고 있음을 의미합니다.

### 6. 다른 알고리즘과 함께 사용하기

- PCA는 다른 기계학습 알고리즘 전에 데이터를 전처리하는 단계로 사용될 수 있습니다. 차원이 축소된 데이터는 학습 속도를 향상시키고, 과적합을 방지하는 효과가 있습니다. 예를 들어, 고차원 데이터에 대해 SVM, 로지스틱 회귀 등을 적용하기 전에 PCA로 차원을 축소하는 것이 일반적입니다.

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# PCA를 적용한 데이터로 로지스틱 회귀 모델을 학습합니다.
X_train, X_test, y_train, y_test = train_test_split(X_pca, data.target, test_size=0.2, random_state=42)
model = LogisticRegression()
model.fit(X_train, y_train)

# 모델의 정확도를 평가합니다.
print("Test accuracy:", model.score(X_test, y_test))
```


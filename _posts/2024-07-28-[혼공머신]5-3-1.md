---
layout: single
title: "[혼공머신]5-3"
categories: machine-learning
tag: [python, machine-learning]
use_math: true
toc: True ##Table of contents
typora-root-url: ../ 
---



### XGBoost와 LightGBM

XGBoost와 LightGBM은 그레이디언트 부스팅의 구현체로, 빠르고 효율적인 알고리즘을 제공합니다. 이들은 특히 대규모 데이터셋에 대해 뛰어난 성능을 발휘합니다.

#### XGBoost

XGBoost는 Extreme Gradient Boosting의 약자로, 효율적이고 확장 가능한 그레이디언트 부스팅 알고리즘입니다. 주요 특징은 다음과 같습니다:

- **Regularization**: 과적합을 방지하기 위한 정규화 기법을 포함합니다.
- **Parallel Processing**: 병렬 처리를 통해 학습 속도를 향상시킵니다.
- **Tree Pruning**: 최적의 트리 크기를 찾기 위해 사후 가지치기를 사용합니다.
- **Sparsity Awareness**: 희소 데이터(예: 결측값)에 대한 최적화를 포함합니다.
- **Cross Validation**: 내부적으로 교차 검증을 통해 모델의 성능을 평가할 수 있습니다.

#### XGBoost 예제 코드

```python
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, cross_validate
import pandas as pd

# 데이터 로드 및 전처리
wine = pd.read_csv('https://bit.ly/wine_csv_data')
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)

# XGBoost 모델 학습
xgb = XGBClassifier(tree_method='hist', random_state=42)
scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

xgb.fit(train_input, train_target)
print(xgb.score(test_input, test_target))
```

#### LightGBM

LightGBM은 Microsoft에서 개발한 그레이디언트 부스팅 프레임워크로, 다음과 같은 특징이 있습니다:

- **Leaf-wise Growth**: 기존의 레벨-wise 방식과 달리, 잎사귀 수준에서 가장 손실이 큰 잎사귀를 먼저 분할합니다. 이로 인해 훈련 속도가 빠르고, 더 나은 성능을 보일 수 있습니다.
- **Histogram-based**: 연속형 특성을 히스토그램으로 변환하여 효율성을 높입니다.
- **Sparse Optimization**: 희소 데이터에 대한 최적화 기법을 포함합니다.
- **Categorical Features**: 범주형 변수를 자동으로 처리합니다.

#### LightGBM 예제 코드

```python
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split, cross_validate
import pandas as pd

# 데이터 로드 및 전처리
wine = pd.read_csv('https://bit.ly/wine_csv_data')
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)

# LightGBM 모델 학습
lgb = LGBMClassifier(random_state=42)
scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

lgb.fit(train_input, train_target)
print(lgb.score(test_input, test_target))
```

### 요약

- **랜덤 포레스트**: 여러 결정 트리를 생성하고, 각 트리의 예측을 결합하여 최종 예측을 만듭니다.
- **엑스트라 트리**: 랜덤 포레스트와 유사하지만, 노드 분할 시 더 많은 무작위성을 도입하여 과적합을 줄입니다.
- **그레이디언트 부스팅**: 각 트리가 이전 트리의 오류를 보완하도록 순차적으로 트리를 추가합니다.
- **히스토그램 기반 그레이디언트 부스팅**: 그레이디언트 부스팅의 개선 버전으로, 히스토그램을 사용하여 학습 속도를 높입니다.
- **XGBoost**: 고효율의 그레이디언트 부스팅 구현체로, 병렬 처리와 정규화 등의 기능을 포함합니다.
- **LightGBM**: 빠르고 확장 가능한 그레이디언트 부스팅 프레임워크로, 잎사귀 수준의 성장과 히스토그램 기반의 최적화를 사용합니다.

각 알고리즘의 장단점을 이해하고 적절히 활용하면 머신러닝 모델의 성능을 극대화할 수 있습니다.
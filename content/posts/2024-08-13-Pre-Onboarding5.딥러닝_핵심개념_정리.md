---
image: "/images/posts/2024-08-13-Pre-Onboarding5/cover.jpg"
categories:
- machine-learning
date: '2024-08-13'
math: true
tags:
- machine-learning
- job-interview
title: '[원티드_프리온보딩]5.딥러닝 핵심개념 정리'
toc: true
---
## 딥러닝 핵심 개념 정리

딥러닝은 인공신경망을 기반으로 하여 대량의 데이터를 처리하고 학습하는 기계 학습의 한 분야입니다. 여러 계층(layer)으로 구성된 신경망을 통해 복잡한 패턴을 학습할 수 있습니다. 아래는 딥러닝의 핵심 개념을 표와 수식, 그래프를 사용하여 자세히 정리한 것입니다.

### 딥러닝의 기본 개념

#### 인공신경망 (Artificial Neural Network)

인공신경망은 뇌의 신경망을 본떠 만든 구조로, 입력 계층, 은닉 계층, 출력 계층으로 구성됩니다. 각 계층은 뉴런(노드)으로 이루어져 있으며, 뉴런 간에는 가중치(weight)가 연결되어 있습니다.

1. **퍼셉트론 (Perceptron)**

   - **구성 요소:** 입력, 가중치, 활성화 함수
   - **수식:** 
     $$
     \text{Output} = f\left(\sum_{i=1}^{n} w_i \cdot x_i + b\right)
     $$
     여기서 $ w_i $는 가중치, $x_i$는 입력값, $b$는 편향(bias), $f$는 활성화 함수입니다.
   
2. **다층 퍼셉트론 (Multi-Layer Perceptron, MLP)**
   
   - 여러 개의 퍼셉트론이 층(layer)으로 연결되어 있는 구조.
   - 입력층, 은닉층(hidden layer), 출력층으로 구성.

#### 활성화 함수 (Activation Function)

활성화 함수는 신경망의 각 뉴런에서 입력 신호를 받아 출력 신호로 변환하는 역할을 하며, 신경망이 학습할 수 있는 비선형성을 제공합니다. 아래는 주요 활성화 함수의 특징과 사용 사례를 정리한 표입니다.

### 활성화 함수의 특징

| 활성화 함수                      | 수식                                                         | 특징 및 사용 사례                                            |
| -------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Sigmoid**                      | $ \sigma(x) = \frac{1}{1 + e^{-x}} $                         | - **특징:** 출력값을 0과 1 사이로 압축하는 S자 형태의 곡선입니다. 연속적인 실수 값을 확률값으로 변환할 때 주로 사용됩니다.<br>- **사용 사례:** 이진 분류 문제에서 출력 뉴런의 활성화 함수로 자주 사용됩니다.<br>- **단점:** 입력 값의 절대값이 큰 경우 기울기가 매우 작아져서, 학습이 느려지는 vanishing gradient 문제를 발생시킬 수 있습니다. |
| **Tanh**                         | $ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $             | - **특징:** Sigmoid와 유사하지만 출력값이 -1과 1 사이로 압축됩니다. 데이터의 중앙값을 0으로 맞추는 데 유리합니다.<br>- **사용 사례:** LSTM 및 RNN과 같은 순환 신경망에서 내부 상태를 업데이트할 때 자주 사용됩니다.<br>- **장점:** Sigmoid보다 중심이 0에 가까워 학습 효율이 높은 편입니다. |
| **ReLU (Rectified Linear Unit)** | $ f(x) = \max(0, x) $                                        | - **특징:** 입력이 양수면 그대로 출력하고, 음수면 0을 출력합니다. 신경망에 비선형성을 부여하면서 계산이 매우 간단합니다.<br>- **사용 사례:** CNN 및 MLP에서 은닉층의 활성화 함수로 널리 사용됩니다.<br>- **장점:** Sigmoid와 Tanh에 비해 vanishing gradient 문제가 덜 발생하며, 학습이 빠릅니다.<br>- **단점:** 입력이 음수인 경우 기울기가 0이 되어 학습이 멈추는 dying ReLU 문제가 발생할 수 있습니다. |
| **Leaky ReLU**                   | $ f(x) = \max(0.01x, x) $                                    | - **특징:** ReLU의 변형으로, 음수 입력에 대해 아주 작은 기울기를 부여합니다.<br>- **사용 사례:** ReLU의 dying 문제를 해결하기 위해 사용됩니다.<br>- **장점:** 모든 입력 값에 대해 기울기가 존재하므로, ReLU보다 안정적인 학습이 가능합니다. |
| **Softmax**                      | $ \sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} $ | - **특징:** 여러 클래스에 대한 확률 분포를 출력하도록 설계되었습니다. 입력 벡터의 각 요소를 확률로 변환합니다.<br>- **사용 사례:** 다중 클래스 분류 문제의 출력층에 사용됩니다.<br>- **장점:** 출력값의 합이 1이 되어 확률 분포를 표현할 수 있습니다. |

각 활성화 함수는 특정 상황에서 유용하게 사용되며, 문제의 특성에 따라 적절한 함수를 선택하는 것이 중요합니다. 활성화 함수를 선택할 때는 학습의 안정성, 계산의 효율성, 문제의 특성을 고려해야 합니다.

#### 손실 함수 (Loss Function)

손실 함수는 모델의 예측과 실제 값 간의 차이를 측정하여 학습 방향을 결정하는 중요한 역할을 합니다.

1. **Mean Squared Error (MSE)**

   - **정의:** 회귀 문제에서 주로 사용되는 손실 함수.
   - **수식:**
     $$
     \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
     $$
     여기서 $y_i$는 실제 값, $\hat{y}_i$는 예측 값입니다.
   
2. **Cross Entropy Loss**

   - **정의:** 분류 문제에서 사용되는 손실 함수.
   - **수식:**
     $$
     \text{Cross Entropy} = -\sum_{i=1}^{n} y_i \log(\hat{y}_i)
     $$


#### 역전파 (Backpropagation)

역전파는 신경망의 가중치를 업데이트하는 알고리즘으로, 손실 함수의 기울기를 계산하여 가중치를 조정합니다.

- **개념:** 출력에서 입력 방향으로 손실의 그래디언트를 전파하여 각 가중치를 업데이트.
- **수식:** 가중치 업데이트 공식
  $$
  w = w - \eta \cdot \frac{\partial \text{Loss}}{\partial w}
  $$
  여기서 $ \eta $는 학습률(learning rate)입니다.

### 딥러닝의 최적화 알고리즘

최적화 알고리즘은 역전파 과정에서 가중치를 효율적으로 업데이트하기 위한 방법을 제공합니다.

| 알고리즘                              | 특징                                                         |
| ------------------------------------- | ------------------------------------------------------------ |
| **SGD (Stochastic Gradient Descent)** | 각 샘플에 대해 가중치를 업데이트하여 학습을 빠르게 진행하지만, 노이즈가 많을 수 있습니다. |
| **Momentum**                          | 이전 단계의 기울기를 고려하여 업데이트, 진동 감소 및 수렴 속도 증가 |
| **Adam**                              | 학습률을 개별적으로 조정하여 빠르고 안정적인 수렴을 유도합니다. |

### 배치 학습과 미니배치 학습

배치 학습과 미니배치 학습은 데이터를 나누어 처리하는 방식으로, 모델의 학습 효율성을 높입니다.

- **배치 학습 (Batch Learning):** 전체 데이터셋을 한 번에 처리하여 가중치를 업데이트합니다.
- **미니배치 학습 (Mini-batch Learning):** 데이터셋을 작은 배치로 나누어 각 배치마다 가중치를 업데이트합니다.
  - **예시:** 데이터셋 크기 = 1000, 배치 크기 = 100일 때, 10번의 업데이트 수행.

### 딥러닝 모델 평가 지표

딥러닝 모델의 성능을 평가하기 위해 다양한 지표를 사용합니다.

1. **정확도 (Accuracy)**
   - **정의:** 전체 예측 중에서 정확히 맞춘 비율.
   - **계산식:** 
     $$
     \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{FP} + \text{FN} + \text{TN}}
     $$

2. **정밀도 (Precision)**
   - **정의:** 양성으로 예측한 것 중 실제 양성의 비율.
   - **계산식:** 
     $$
     \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
     $$

3. **재현율 (Recall)**
   - **정의:** 실제 양성 중에서 양성으로 예측한 비율.
   - **계산식:** 
     $$
     \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
     $$

4. **F1-score**
   - **정의:** 정밀도와 재현율의 조화 평균.
   - **계산식:** 
     $$
     \text{F1-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
     $$


딥러닝은 복잡한 데이터 구조를 학습하고 추론하는 강력한 도구이며, 이러한 개념과 기술을 이해함으로써 더욱 효과적으로 적용할 수 있습니다. 딥러닝 모델을 구축하고 평가하는 과정에서 이러한 핵심 개념을 잘 활용하는 것이 중요합니다.